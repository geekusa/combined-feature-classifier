{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import re\n",
    "import mailparser\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tld import get_tld\n",
    "from time import strftime, strptime, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, recall_score\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set constants for directories, take a random sampling of the \"ignore\" email since there are so many (under sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_DIR = '/Users/worshamn/Documents/emailProject/IgnoreFix'\n",
    "INVESTIGATE_DIR = '/Users/worshamn/Documents/emailProject/InvestigateFix'\n",
    "#https://stackoverflow.com/a/6482922\n",
    "random.seed(2842)\n",
    "ignore_sample_index = [ i for i in sorted(random.sample(range(len(os.listdir(IGNORE_DIR))), 400)) ]\n",
    "ignore_sample = []\n",
    "for i in ignore_sample_index:\n",
    "    ignore_sample.append(os.listdir(IGNORE_DIR)[i])\n",
    "input_dirs = {}\n",
    "input_dirs[INVESTIGATE_DIR] = os.listdir(INVESTIGATE_DIR) \n",
    "input_dirs[IGNORE_DIR] = ignore_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Phish Alert] FW- Barbara.jones shrink your belly, 1lb day.eml',\n",
       "...\n",
       " '[Phish Alert] FW- How 4,500 Worksites Are Getting Healthier Right Now.eml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(input_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_dirs[INVESTIGATE_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_dirs[IGNORE_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to extract text and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_text(file):\n",
    "    d = {}\n",
    "    raw_message = email.message_from_file(file)\n",
    "    mail = mailparser.parse_from_string(raw_message.as_string())\n",
    "    d['subject'] = mail.subject\n",
    "    d['subject_len'] = len(d['subject'])\n",
    "    if raw_message.is_multipart():\n",
    "        d['is_mulitpart'] = 1\n",
    "    else:\n",
    "        d['is_multipart'] = 0\n",
    "    d['body'] = mail.text_plain\n",
    "    if len(d['body']) > 0:\n",
    "        d['mail_text'] = d['subject'] + ' ' + d['body'][0]\n",
    "        d['body_len'] = len(d['body'][0])\n",
    "        if len(d['body']) > 1:\n",
    "            soup_html = BeautifulSoup(d['body'][1],'lxml')\n",
    "            d['links'] = soup_html.find_all('a')\n",
    "            d['num_link'] = len(d['links'])\n",
    "            links = []\n",
    "            d['masq_link'] = []\n",
    "            d['masq_link_tld'] = []\n",
    "            d['num_email_link'] = 0\n",
    "            for link in d['links']:\n",
    "                link_text = link.get_text().rstrip('\\n')\n",
    "                a_link = link.get('href')\n",
    "                links.append(a_link)\n",
    "                if 'unsubscribe' in link_text.lower():\n",
    "                    d['has_unsubscribe_link'] = 1\n",
    "                if a_link:    \n",
    "                    if re.search('mailto:',a_link):\n",
    "                        d['num_email_link'] += 1\n",
    "                if a_link != link_text and \\\n",
    "                    'http' in link_text.lower() and \\\n",
    "                    not 'alt=\"http' in link_text.lower():\n",
    "                        d['masq_link'].append(link)\n",
    "                        d['masq_link_tld'].append(\n",
    "                            get_tld(\n",
    "                                a_link,\n",
    "                                fix_protocol=True, \n",
    "                                fail_silently=True\n",
    "                            )\n",
    "                        )\n",
    "            d['num_uniq_link'] = len(set(links))\n",
    "            if d['num_link'] > d['num_uniq_link']:\n",
    "                d['has_repeatlink'] = 1\n",
    "            else:\n",
    "                d['has_repeatlink'] = 0\n",
    "            if len(d['masq_link']) == 0:\n",
    "                d['masq_link'] = ''\n",
    "                d['masq_link_tld'] = ''\n",
    "                d['has_masq_link'] = 0\n",
    "            else:    \n",
    "                d['has_masq_link'] = 1\n",
    "                d['num_masq_link'] = len(d['masq_link'])\n",
    "    else:\n",
    "        d['mail_text'] = d['subject']\n",
    "        d['body_len'] = len(d['body'])\n",
    "    url_query = '((?:https?|ftp)://[^\\s/$.?#]+\\.[^\\s>]+)'\n",
    "    d['url'] = re.findall(url_query,d['mail_text'])\n",
    "    email_query = '([\\w.]+@[\\w.]+\\.[\\w.]{2,5})'\n",
    "    d['email'] = re.findall(email_query,d['mail_text'])\n",
    "    if d['url']:\n",
    "        d['has_url'] = 1\n",
    "        d['num_url'] = len(d['url'])\n",
    "        d['num_uniq_url'] = len(set(d['url']))\n",
    "        d['num_url_repeats'] = d['num_url'] - d['num_uniq_url']\n",
    "        d['url_len'] = []\n",
    "        d['url_tld'] = []\n",
    "        for i in d['url']:\n",
    "            d['url_len'].append(len(i))\n",
    "            d['url_tld'].append(\n",
    "                get_tld(i, fix_protocol=True, fail_silently=True)\n",
    "            )\n",
    "            d['uniq_url_tld'] = set(d['url_tld'])\n",
    "    else:\n",
    "        d['url'] = ''\n",
    "        d['has_url'] = 0\n",
    "        d['num_url'] = 0\n",
    "        d['num_uniq_url'] = 0\n",
    "        d['url_len'] = 0\n",
    "        d['url_tld'] = 0\n",
    "        d['uniq_url_tld'] = 0\n",
    "        d['num_url_repeats'] = 0\n",
    "    if d['email']:\n",
    "        d['has_email'] = 1\n",
    "        d['num_email_addr'] = len(d['email'])\n",
    "        d['num_uniq_email'] = len(set(d['email']))\n",
    "    else:\n",
    "        d['email'] = ''\n",
    "        d['has_email'] = 0\n",
    "        d['num_email_addr'] = 0\n",
    "        d['num_uniq_email'] = 0\n",
    "    soup = BeautifulSoup(d['mail_text'],'lxml')\n",
    "    try:\n",
    "        d['mail_text'] = soup.get_text().encode(\n",
    "            'utf-8',\n",
    "            'ignore'\n",
    "        ).decode('unicode_escape').replace('\\n',' ').replace('\\t',' ')\n",
    "    except:\n",
    "        d['mail_text'] = soup.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "    d['reply_to'] = mail.reply_to\n",
    "    for k,v in mail.headers.items():\n",
    "        d[k] = v\n",
    "    d['body'] = mail.body\n",
    "    d['text_plain'] = mail.text_plain\n",
    "    if mail.attachments:\n",
    "        d['has_attachments'] = 1\n",
    "        d['num_attachments'] = len(mail.attachments)\n",
    "        for i in mail.attachments:\n",
    "            try:\n",
    "                d['attachment_filename'].append(i['filename'])\n",
    "            except:\n",
    "                d['attachment_filename'] = []\n",
    "                d['attachment_filename'].append(i['filename'])\n",
    "            try:\n",
    "                d['attachment_content_type'].append(i['mail_content_type'])\n",
    "            except:\n",
    "                d['attachment_content_type'] = []\n",
    "                d['attachment_content_type'].append(i['mail_content_type'])\n",
    "    else:\n",
    "        d['has_attachments'] = 0\n",
    "    if re.search(tryGetKeyValue(d, 'Return-Path'),tryGetKeyValue(d, 'From')):\n",
    "        d['return_path_match_from'] = 1\n",
    "    else:\n",
    "        d['return_path_match_from'] = 0\n",
    "    tld_match = re.match(\n",
    "            '[^@]+@([^>]+)>',\n",
    "            tryGetKeyValue(d, 'From')\n",
    "        )\n",
    "    if tld_match:\n",
    "        d['from_tld'] = get_tld(\n",
    "            tld_match.group(1),\n",
    "            fix_protocol=True, \n",
    "            fail_silently=True\n",
    "        )\n",
    "    if 'content=\"text/html' in d['body'].lower():\n",
    "        d['has_html_content'] = 1\n",
    "    else:\n",
    "        d['has_html_content'] = 0\n",
    "    if 'script type=\"text/javascript' in d['body'].lower():\n",
    "        d['has_javascript'] = 1\n",
    "    else:\n",
    "        d['has_javascript'] = 0\n",
    "    if 'img src=\"cid:' in d['body'].lower():\n",
    "        d['has_inline_img'] = 1\n",
    "    else:\n",
    "        d['has_inline_img'] = 0\n",
    "    if 'Content-type' in d:\n",
    "        d['Content-type'] = re.match('([^;]+);',d['Content-type']).group(1)\n",
    "    else:\n",
    "        d['Content-type'] = None\n",
    "    if 'Date' in d:\n",
    "        #d['DOTW'] = strftime('%a',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "        d['DOTW'] = strftime('%w',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "        d['HOTD'] = strftime('%H',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "    if mail.has_defects:\n",
    "        d['has_defects'] = 1\n",
    "    else:\n",
    "        d['has_defects'] = 0\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to set the target value based on the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(d):\n",
    "    if d == IGNORE_DIR:\n",
    "        return 'ignore'\n",
    "    elif d == INVESTIGATE_DIR:\n",
    "        return 'investigate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to extract value only if the key exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryGetKeyValue(d, key, return_value=''):\n",
    "  \"\"\"Attempts to return value of key from dictionary\n",
    "  \"\"\"\n",
    "  try:\n",
    "    return d[key]\n",
    "  except:\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe, iterate through directories and add email features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/worshamn/Documents/emailProject/InvestigateFix [Phish Alert] FW- Goodchem Document.eml\n",
       "...\n",
      "/Users/worshamn/Documents/emailProject/IgnoreFix [Phish Alert] FW- How 4,500 Worksites Are Getting Healthier Right Now.eml\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for d,v in input_dirs.items():\n",
    "    for f in input_dirs[d]:\n",
    "        print(d, f)\n",
    "        with open(os.path.join(d,f),'r',encoding='utf-8', errors='ignore') as raw_mail:\n",
    "            mail_dict = get_email_text(raw_mail)\n",
    "            df = df.append(\n",
    "                {\n",
    "                    #'filename': f,\n",
    "                    'text': mail_dict['mail_text'],\n",
    "                    'target': get_target(d),\n",
    "                    'subject_len': mail_dict['subject_len'],\n",
    "                    'body_len': mail_dict['body_len'],\n",
    "                    'has_attachments': tryGetKeyValue(mail_dict, 'has_attachments',0),\n",
    "                    'num_attachments': tryGetKeyValue(mail_dict, 'num_attachments',0),\n",
    "                    #'attachment_filename': tryGetKeyValue(mail_dict, 'attachment_filename'),\n",
    "                    #'attachment_content_type': tryGetKeyValue(mail_dict, 'attachment_content_type'),\n",
    "                    'DKIM': tryGetKeyValue(mail_dict, 'X-BAEAI-DKIM'),\n",
    "                    'DMARC': tryGetKeyValue(mail_dict, 'X-BAEAI-DMARC'),\n",
    "                    'SPF': tryGetKeyValue(mail_dict, 'X-BAEAI-SPF'),\n",
    "                    'return_path_match_from': mail_dict['return_path_match_from'],\n",
    "                    'from_tld': tryGetKeyValue(mail_dict, 'from_tld'),\n",
    "                    'Content-type': mail_dict['Content-type'],\n",
    "                    'DOTW': tryGetKeyValue(mail_dict, 'DOTW'),\n",
    "                    'HOTD': tryGetKeyValue(mail_dict, 'HOTD'),\n",
    "                    #'url': mail_dict['url'],\n",
    "                    'has_url': tryGetKeyValue(mail_dict, 'has_url',0),\n",
    "                    'num_url': tryGetKeyValue(mail_dict, 'num_url',0),\n",
    "                    'num_uniq_url': tryGetKeyValue(mail_dict, 'num_uniq_url',0),\n",
    "                    #'email': tryGetKeyValue(mail_dict, 'email'),\n",
    "                    'has_email': tryGetKeyValue(mail_dict, 'has_email',0),\n",
    "                    'num_email_addr': tryGetKeyValue(mail_dict, 'num_email_addr',0),\n",
    "                    'num_uniq_email': tryGetKeyValue(mail_dict, 'num_uniq_email',0),\n",
    "                    'num_url_repeats': tryGetKeyValue(mail_dict, 'num_url_repeats',0),\n",
    "                    #'url_len': mail_dict['url_len'],\n",
    "                    #'url_tld': mail_dict['url_tld'],\n",
    "                    #'uniq_url_tld': mail_dict['uniq_url_tld'],\n",
    "                    'has_html_content': tryGetKeyValue(mail_dict, 'has_html_content',0),\n",
    "                    'has_javascript': tryGetKeyValue(mail_dict, 'has_javascript',0),\n",
    "                    'has_inline_img': tryGetKeyValue(mail_dict, 'has_inline_img',0),\n",
    "                    'TAP-Score': tryGetKeyValue(mail_dict, 'X-USANET-TAP-Score',-1),\n",
    "                    #'links': tryGetKeyValue(mail_dict, 'links'),\n",
    "                    'num_link': tryGetKeyValue(mail_dict, 'num_link',0),\n",
    "                    'num_uniq_link': tryGetKeyValue(mail_dict, 'num_uniq_link',0),\n",
    "                    'has_repeat_link': tryGetKeyValue(mail_dict, 'has_repeat_link',0),\n",
    "                    #'masq_link': tryGetKeyValue(mail_dict, 'masq_link'),\n",
    "                    'has_masq_link': tryGetKeyValue(mail_dict, 'has_masq_link',0),\n",
    "                    'num_masq_link': tryGetKeyValue(mail_dict, 'num_masq_link',0),\n",
    "                    #'masq_link_tld': tryGetKeyValue(mail_dict, 'masq_link_tld'),\n",
    "                    'is_multipart': tryGetKeyValue(mail_dict, 'is_mulitpart', 0),\n",
    "                    'has_defects': mail_dict['has_defects'],\n",
    "                    'num_email_link': tryGetKeyValue(mail_dict, 'num_email_link',0),\n",
    "                    'has_unsubscribe_link': tryGetKeyValue(mail_dict, 'has_unsubscribe_link', 0),\n",
    "                }, \n",
    "                ignore_index=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 34 columns):\n",
      "Content-type              773 non-null object\n",
      "DKIM                      784 non-null object\n",
      "DMARC                     784 non-null object\n",
      "DOTW                      784 non-null object\n",
      "HOTD                      784 non-null object\n",
      "SPF                       784 non-null object\n",
      "TAP-Score                 784 non-null object\n",
      "body_len                  784 non-null float64\n",
      "from_tld                  784 non-null object\n",
      "has_attachments           784 non-null float64\n",
      "has_defects               784 non-null float64\n",
      "has_email                 784 non-null float64\n",
      "has_html_content          784 non-null float64\n",
      "has_inline_img            784 non-null float64\n",
      "has_javascript            784 non-null float64\n",
      "has_masq_link             784 non-null float64\n",
      "has_repeat_link           784 non-null float64\n",
      "has_unsubscribe_link      784 non-null float64\n",
      "has_url                   784 non-null float64\n",
      "is_multipart              784 non-null float64\n",
      "num_attachments           784 non-null float64\n",
      "num_email_addr            784 non-null float64\n",
      "num_email_link            784 non-null float64\n",
      "num_link                  784 non-null float64\n",
      "num_masq_link             784 non-null float64\n",
      "num_uniq_email            784 non-null float64\n",
      "num_uniq_link             784 non-null float64\n",
      "num_uniq_url              784 non-null float64\n",
      "num_url                   784 non-null float64\n",
      "num_url_repeats           784 non-null float64\n",
      "return_path_match_from    784 non-null float64\n",
      "subject_len               784 non-null float64\n",
      "target                    784 non-null object\n",
      "text                      784 non-null object\n",
      "dtypes: float64(24), object(10)\n",
      "memory usage: 208.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    docs, \n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    lemmatize=True,\n",
    "    remove_stopwords=True, \n",
    "    custom_stopwords=None,\n",
    "#     term_min_len=0,\n",
    "):\n",
    "    #only use parts of spaCy needed\n",
    "    nlp = spacy.load('en', disable=['parser','ner','textcat'])\n",
    "    #remove urls\n",
    "    if remove_urls:\n",
    "        print('remove URLS')\n",
    "        docs = [\n",
    "            re.sub('(?i)(?:www|https?)(?:://)?[^\\s]+','',text)\n",
    "            for text in docs\n",
    "        ]\n",
    "    #remove emails\n",
    "    if remove_emails:\n",
    "        print('remove email addresses')\n",
    "        docs = [\n",
    "            re.sub('(?i)[\\w.]+@[\\w.]+\\.[\\w.]{2,5}','',text)\n",
    "            for text in docs\n",
    "        ]\n",
    "    #remove punct and digits\n",
    "    print('removing punctuation and digits and change to lowercase')\n",
    "    table = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "    clean_docs = [\n",
    "        str(d).lower().translate(table)\n",
    "        for d in docs\n",
    "    ]\n",
    "        \n",
    "    #tokenize in spacy\n",
    "    if lemmatize:\n",
    "        print('spacy tokenization')\n",
    "        nlp_docs = [nlp(d) for d in clean_docs]\n",
    "        #lemmatization, words like I get changed into -PRON- so leave them alone\n",
    "        if remove_stopwords:\n",
    "            print('lemmatization and remove stopwords')\n",
    "            if custom_stopwords:\n",
    "                custom_stopwords = set(custom_stopwords)\n",
    "            else:\n",
    "                custom_stopwords = []\n",
    "            lemmatized_docs = [\n",
    "                [\n",
    "                    w.lemma_ \n",
    "                    for w in d\n",
    "                    if (w.lemma_ != '-PRON-' and not w.is_stop and w.lemma_ not in custom_stopwords) \n",
    "                ]\n",
    "                for d in nlp_docs\n",
    "            ]\n",
    "        else:\n",
    "            print('lemmatization')\n",
    "            lemmatized_docs = [\n",
    "            [\n",
    "                w.lemma_\n",
    "                if w.lemma_ != '-PRON-'\n",
    "                else w.lower_\n",
    "                for w in d\n",
    "                #if (w.lemma_ != '-PRON-' and len(w.lemma_)>term_min_len)\n",
    "            ]\n",
    "            for d in nlp_docs\n",
    "        ]\n",
    "    if lemmatized_docs:\n",
    "        clean_docs = lemmatized_docs\n",
    "    \n",
    "    # join tokens back into doc\n",
    "    clean_docs = [\n",
    "        ' '.join(l) \n",
    "        for l in clean_docs\n",
    "    ]\n",
    "\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove URLS\n",
      "remove email addresses\n",
      "removing punctuation and digits and change to lowercase\n",
      "spacy tokenization\n",
      "lemmatization and remove stopwords\n",
      "done in 8.711s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "corpus = clean_text(\n",
    "    df['text'], \n",
    ")\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = pd.Series(corpus).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert cells that are not recognized as a number or just to get rid of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/28910914\n",
    "for col in [\n",
    "    'body_len',\n",
    "    'has_attachments',\n",
    "    'has_defects',\n",
    "    'has_email',\n",
    "    'has_html_content',\n",
    "    'has_inline_img',\n",
    "    'has_javascript',\n",
    "    'has_masq_link',\n",
    "    'has_repeat_link',\n",
    "    'has_unsubscribe_link',\n",
    "    'has_url',\n",
    "    'is_multipart',\n",
    "    'num_attachments',\n",
    "    'num_email_addr',\n",
    "    'num_email_link',\n",
    "    'num_link',\n",
    "    'num_masq_link',\n",
    "    'num_uniq_email',\n",
    "    'num_uniq_link',\n",
    "    'num_uniq_url',\n",
    "    'num_url',\n",
    "    'num_url_repeats',\n",
    "    'return_path_match_from',\n",
    "    'subject_len',\n",
    "    #'TAP-Score',\n",
    "]:\n",
    "    df[col] = df[col].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 35 columns):\n",
      "Content-type              773 non-null object\n",
      "DKIM                      784 non-null object\n",
      "DMARC                     784 non-null object\n",
      "DOTW                      784 non-null object\n",
      "HOTD                      784 non-null object\n",
      "SPF                       784 non-null object\n",
      "TAP-Score                 784 non-null object\n",
      "body_len                  784 non-null int64\n",
      "from_tld                  784 non-null object\n",
      "has_attachments           784 non-null int64\n",
      "has_defects               784 non-null int64\n",
      "has_email                 784 non-null int64\n",
      "has_html_content          784 non-null int64\n",
      "has_inline_img            784 non-null int64\n",
      "has_javascript            784 non-null int64\n",
      "has_masq_link             784 non-null int64\n",
      "has_repeat_link           784 non-null int64\n",
      "has_unsubscribe_link      784 non-null int64\n",
      "has_url                   784 non-null int64\n",
      "is_multipart              784 non-null int64\n",
      "num_attachments           784 non-null int64\n",
      "num_email_addr            784 non-null int64\n",
      "num_email_link            784 non-null int64\n",
      "num_link                  784 non-null int64\n",
      "num_masq_link             784 non-null int64\n",
      "num_uniq_email            784 non-null int64\n",
      "num_uniq_link             784 non-null int64\n",
      "num_uniq_url              784 non-null int64\n",
      "num_url                   784 non-null int64\n",
      "num_url_repeats           784 non-null int64\n",
      "return_path_match_from    784 non-null int64\n",
      "subject_len               784 non-null int64\n",
      "target                    784 non-null object\n",
      "text                      784 non-null object\n",
      "cleaned_text              784 non-null object\n",
      "dtypes: int64(24), object(11)\n",
      "memory usage: 214.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the continous data into ranges for binary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/40548606\n",
    "df['body_len'] = pd.qcut(df['body_len'],20,duplicates='drop')\n",
    "df['subject_len'] = pd.qcut(df['subject_len'],10,duplicates='drop')\n",
    "df['num_attachments'] = pd.cut(df['num_attachments'],5)\n",
    "df['num_email_addr'] = pd.cut(df['num_email_addr'],5)\n",
    "df['num_email_link'] = pd.cut(df['num_email_link'],5)\n",
    "df['num_link'] = pd.qcut(df['num_link'],10,duplicates='drop')\n",
    "df['num_masq_link'] = pd.cut(df['num_masq_link'],3)\n",
    "df['num_uniq_email'] = pd.qcut(df['num_uniq_email'],10,duplicates='drop')\n",
    "df['num_uniq_url'] = pd.qcut(df['num_uniq_url'],10,duplicates='drop')\n",
    "df['num_url'] = pd.qcut(df['num_url'],10,duplicates='drop')\n",
    "df['num_url_repeats'] = pd.qcut(df['num_url_repeats'],10,duplicates='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the continuous and categorical data into one-hot encoding (binary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'DKIM',\n",
    "    'DMARC',\n",
    "    'SPF',\n",
    "    'from_tld',\n",
    "    'Content-type',\n",
    "    'DOTW',\n",
    "    'HOTD',\n",
    "    'TAP-Score',\n",
    "    'body_len',\n",
    "    'subject_len',\n",
    "    'num_attachments',\n",
    "    'num_email_addr',\n",
    "    'num_email_link',\n",
    "    'num_link',\n",
    "    'num_masq_link',\n",
    "    'num_uniq_email',\n",
    "    'num_uniq_url',\n",
    "    'num_url',\n",
    "    'num_url_repeats',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = pd.get_dummies(df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(categorical_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_categorical], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_attachments</th>\n",
       "      <th>has_defects</th>\n",
       "      <th>has_email</th>\n",
       "      <th>has_html_content</th>\n",
       "      <th>has_inline_img</th>\n",
       "      <th>has_javascript</th>\n",
       "      <th>has_masq_link</th>\n",
       "      <th>has_repeat_link</th>\n",
       "      <th>has_unsubscribe_link</th>\n",
       "      <th>has_url</th>\n",
       "      <th>...</th>\n",
       "      <th>num_url_(-0.001, 1.0]</th>\n",
       "      <th>num_url_(1.0, 2.0]</th>\n",
       "      <th>num_url_(2.0, 4.0]</th>\n",
       "      <th>num_url_(4.0, 6.0]</th>\n",
       "      <th>num_url_(6.0, 11.0]</th>\n",
       "      <th>num_url_(11.0, 17.0]</th>\n",
       "      <th>num_url_(17.0, 341.0]</th>\n",
       "      <th>num_url_repeats_(-0.001, 1.0]</th>\n",
       "      <th>num_url_repeats_(1.0, 3.0]</th>\n",
       "      <th>num_url_repeats_(3.0, 95.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_attachments  has_defects  has_email  has_html_content  has_inline_img  \\\n",
       "0                1            0          1                 0               0   \n",
       "1                0            0          0                 1               0   \n",
       "2                0            0          0                 1               0   \n",
       "3                0            0          0                 0               0   \n",
       "4                0            0          0                 0               0   \n",
       "\n",
       "   has_javascript  has_masq_link  has_repeat_link  has_unsubscribe_link  \\\n",
       "0               0              0                0                     0   \n",
       "1               0              0                0                     0   \n",
       "2               0              1                0                     0   \n",
       "3               0              0                0                     0   \n",
       "4               0              0                0                     0   \n",
       "\n",
       "   has_url             ...               num_url_(-0.001, 1.0]  \\\n",
       "0        1             ...                                   0   \n",
       "1        1             ...                                   0   \n",
       "2        1             ...                                   0   \n",
       "3        1             ...                                   1   \n",
       "4        1             ...                                   1   \n",
       "\n",
       "   num_url_(1.0, 2.0]  num_url_(2.0, 4.0] num_url_(4.0, 6.0]  \\\n",
       "0                   0                   0                  0   \n",
       "1                   1                   0                  0   \n",
       "2                   1                   0                  0   \n",
       "3                   0                   0                  0   \n",
       "4                   0                   0                  0   \n",
       "\n",
       "  num_url_(6.0, 11.0] num_url_(11.0, 17.0]  num_url_(17.0, 341.0]  \\\n",
       "0                   1                    0                      0   \n",
       "1                   0                    0                      0   \n",
       "2                   0                    0                      0   \n",
       "3                   0                    0                      0   \n",
       "4                   0                    0                      0   \n",
       "\n",
       "   num_url_repeats_(-0.001, 1.0]  num_url_repeats_(1.0, 3.0]  \\\n",
       "0                              1                           0   \n",
       "1                              1                           0   \n",
       "2                              1                           0   \n",
       "3                              1                           0   \n",
       "4                              1                           0   \n",
       "\n",
       "   num_url_repeats_(3.0, 95.0]  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_attachments\n",
      "has_defects\n",
      "has_email\n",
      "has_html_content\n",
      "has_inline_img\n",
      "has_javascript\n",
      "has_masq_link\n",
      "has_repeat_link\n",
      "has_unsubscribe_link\n",
      "has_url\n",
      "is_multipart\n",
      "num_uniq_link\n",
      "return_path_match_from\n",
      "target\n",
      "text\n",
      "cleaned_text\n",
      "DKIM_\n",
      "DKIM_FAIL\n",
      "DKIM_NONE\n",
      "DKIM_PASS\n",
      "DMARC_\n",
      "DMARC_absent\n",
      "DMARC_none\n",
      "DMARC_pass\n",
      "DMARC_quarantine\n",
      "DMARC_reject\n",
      "SPF_\n",
      "SPF_ERROR\n",
      "SPF_NEUTRAL\n",
      "SPF_NONE\n",
      "SPF_PASS\n",
      "SPF_SOFTFAIL\n",
      "from_tld_\n",
      "from_tld_ac.uk\n",
      "from_tld_bg\n",
      "from_tld_bid\n",
      "from_tld_biz\n",
      "from_tld_ca\n",
      "from_tld_ch\n",
      "from_tld_co\n",
      "from_tld_co.jp\n",
      "from_tld_co.uk\n",
      "from_tld_co.za\n",
      "from_tld_com\n",
      "from_tld_com.ar\n",
      "from_tld_com.br\n",
      "from_tld_com.do\n",
      "from_tld_com.ec\n",
      "from_tld_com.mx\n",
      "from_tld_com.my\n",
      "from_tld_com.pk\n",
      "from_tld_de\n",
      "from_tld_edu\n",
      "from_tld_email\n",
      "from_tld_es\n",
      "from_tld_eu\n",
      "from_tld_fi\n",
      "from_tld_fr\n",
      "from_tld_gob.cl\n",
      "from_tld_gob.ec\n",
      "from_tld_gov\n",
      "from_tld_gr\n",
      "from_tld_guru\n",
      "from_tld_hu\n",
      "from_tld_ie\n",
      "from_tld_info\n",
      "from_tld_io\n",
      "from_tld_it\n",
      "from_tld_k12.in.us\n",
      "from_tld_k12.wi.us\n",
      "from_tld_mx\n",
      "from_tld_ne.jp\n",
      "from_tld_net\n",
      "from_tld_net.pl\n",
      "from_tld_net.tw\n",
      "from_tld_org\n",
      "from_tld_org.br\n",
      "from_tld_ph\n",
      "from_tld_pl\n",
      "from_tld_ro\n",
      "from_tld_ru\n",
      "from_tld_se\n",
      "from_tld_sk\n",
      "from_tld_travel\n",
      "from_tld_tv\n",
      "from_tld_uk\n",
      "from_tld_us\n",
      "Content-type_Multipart/alternative\n",
      "Content-type_Multipart/mixed\n",
      "Content-type_Multipart/related\n",
      "Content-type_text/plain\n",
      "DOTW_\n",
      "DOTW_0\n",
      "DOTW_1\n",
      "DOTW_2\n",
      "DOTW_3\n",
      "DOTW_4\n",
      "DOTW_5\n",
      "DOTW_6\n",
      "HOTD_\n",
      "HOTD_01\n",
      "HOTD_02\n",
      "HOTD_03\n",
      "HOTD_04\n",
      "HOTD_05\n",
      "HOTD_06\n",
      "HOTD_07\n",
      "HOTD_08\n",
      "HOTD_09\n",
      "HOTD_10\n",
      "HOTD_11\n",
      "HOTD_12\n",
      "HOTD_13\n",
      "HOTD_14\n",
      "HOTD_15\n",
      "HOTD_16\n",
      "HOTD_17\n",
      "HOTD_18\n",
      "HOTD_19\n",
      "HOTD_20\n",
      "HOTD_21\n",
      "HOTD_22\n",
      "HOTD_23\n",
      "TAP-Score_-1\n",
      "TAP-Score_0\n",
      "TAP-Score_1\n",
      "TAP-Score_2\n",
      "TAP-Score_3\n",
      "body_len_(8.999, 111.15]\n",
      "body_len_(111.15, 166.3]\n",
      "body_len_(166.3, 207.45]\n",
      "body_len_(207.45, 237.6]\n",
      "body_len_(237.6, 273.75]\n",
      "body_len_(273.75, 335.9]\n",
      "body_len_(335.9, 422.1]\n",
      "body_len_(422.1, 513.2]\n",
      "body_len_(513.2, 679.15]\n",
      "body_len_(679.15, 949.5]\n",
      "body_len_(949.5, 1179.0]\n",
      "body_len_(1179.0, 1386.6]\n",
      "body_len_(1386.6, 1640.7]\n",
      "body_len_(1640.7, 2000.2]\n",
      "body_len_(2000.2, 2354.5]\n",
      "body_len_(2354.5, 2861.6]\n",
      "body_len_(2861.6, 3923.2]\n",
      "body_len_(3923.2, 5470.1]\n",
      "body_len_(5470.1, 8140.15]\n",
      "body_len_(8140.15, 276506.0]\n",
      "subject_len_(-0.001, 13.0]\n",
      "subject_len_(13.0, 19.0]\n",
      "subject_len_(19.0, 24.0]\n",
      "subject_len_(24.0, 30.0]\n",
      "subject_len_(30.0, 35.0]\n",
      "subject_len_(35.0, 40.0]\n",
      "subject_len_(40.0, 46.0]\n",
      "subject_len_(46.0, 53.0]\n",
      "subject_len_(53.0, 62.0]\n",
      "subject_len_(62.0, 246.0]\n",
      "num_attachments_(-0.015, 3.0]\n",
      "num_attachments_(3.0, 6.0]\n",
      "num_attachments_(6.0, 9.0]\n",
      "num_attachments_(9.0, 12.0]\n",
      "num_attachments_(12.0, 15.0]\n",
      "num_email_addr_(-0.193, 38.6]\n",
      "num_email_addr_(38.6, 77.2]\n",
      "num_email_addr_(77.2, 115.8]\n",
      "num_email_addr_(115.8, 154.4]\n",
      "num_email_addr_(154.4, 193.0]\n",
      "num_email_link_(-0.103, 20.6]\n",
      "num_email_link_(20.6, 41.2]\n",
      "num_email_link_(41.2, 61.8]\n",
      "num_email_link_(61.8, 82.4]\n",
      "num_email_link_(82.4, 103.0]\n",
      "num_link_(-0.001, 1.0]\n",
      "num_link_(1.0, 2.0]\n",
      "num_link_(2.0, 3.0]\n",
      "num_link_(3.0, 6.0]\n",
      "num_link_(6.0, 9.0]\n",
      "num_link_(9.0, 14.0]\n",
      "num_link_(14.0, 339.0]\n",
      "num_masq_link_(-0.002, 0.667]\n",
      "num_masq_link_(0.667, 1.333]\n",
      "num_masq_link_(1.333, 2.0]\n",
      "num_uniq_email_(-0.001, 1.0]\n",
      "num_uniq_email_(1.0, 2.0]\n",
      "num_uniq_email_(2.0, 94.0]\n",
      "num_uniq_url_(-0.001, 1.0]\n",
      "num_uniq_url_(1.0, 2.0]\n",
      "num_uniq_url_(2.0, 3.0]\n",
      "num_uniq_url_(3.0, 6.0]\n",
      "num_uniq_url_(6.0, 9.0]\n",
      "num_uniq_url_(9.0, 15.0]\n",
      "num_uniq_url_(15.0, 341.0]\n",
      "num_url_(-0.001, 1.0]\n",
      "num_url_(1.0, 2.0]\n",
      "num_url_(2.0, 4.0]\n",
      "num_url_(4.0, 6.0]\n",
      "num_url_(6.0, 11.0]\n",
      "num_url_(11.0, 17.0]\n",
      "num_url_(17.0, 341.0]\n",
      "num_url_repeats_(-0.001, 1.0]\n",
      "num_url_repeats_(1.0, 3.0]\n",
      "num_url_repeats_(3.0, 95.0]\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sample set into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df['target']\n",
    "df_feats = df.drop(['target','text'],axis=1)\n",
    "train_feats, test_feats, train_labels, test_labels = train_test_split(\n",
    "    df_feats, \n",
    "    df_target, \n",
    "    test_size=0.20, \n",
    "    random_state=7350\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627, 201)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 201)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ignore         318\n",
       "investigate    309\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.434s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    #max_df=0.9, \n",
    "    min_df=2,\n",
    "    use_idf=False,\n",
    "    binary=True,\n",
    ")\n",
    "train_feats_text = vectorizer.fit_transform(train_feats['cleaned_text'].values.tolist())\n",
    "test_feats_text = vectorizer.transform(test_feats['cleaned_text'].values.tolist()) \n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627, 8943)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 8943)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change TFIDF back to dataframe so it can be concatinated with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/50624143\n",
    "train_feats_text_df = pd.DataFrame(train_feats_text.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbuchung</th>\n",
       "      <th>abend</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able access</th>\n",
       "      <th>able hold</th>\n",
       "      <th>able hold phone</th>\n",
       "      <th>absent</th>\n",
       "      <th>...</th>\n",
       "      <th>youthful beauty</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube google</th>\n",
       "      <th>youtube instagram</th>\n",
       "      <th>yu</th>\n",
       "      <th>yð¾u</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip code</th>\n",
       "      <th>zipongo</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8943 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa   ab  abbuchung  abend  ability  able  able access  able hold  \\\n",
       "0  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "1  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "2  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "3  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "4  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "\n",
       "   able hold phone  absent  ...   youthful beauty  youtube  youtube google  \\\n",
       "0              0.0     0.0  ...               0.0      0.0             0.0   \n",
       "1              0.0     0.0  ...               0.0      0.0             0.0   \n",
       "2              0.0     0.0  ...               0.0      0.0             0.0   \n",
       "3              0.0     0.0  ...               0.0      0.0             0.0   \n",
       "4              0.0     0.0  ...               0.0      0.0             0.0   \n",
       "\n",
       "   youtube instagram   yu  yð¾u  zip  zip code  zipongo  zoom  \n",
       "0                0.0  0.0   0.0  0.0       0.0      0.0   0.0  \n",
       "1                0.0  0.0   0.0  0.0       0.0      0.0   0.0  \n",
       "2                0.0  0.0   0.0  0.0       0.0      0.0   0.0  \n",
       "3                0.0  0.0   0.0  0.0       0.0      0.0   0.0  \n",
       "4                0.0  0.0   0.0  0.0       0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 8943 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(627, 9143)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats = pd.concat([\n",
    "    train_feats.reset_index(drop=True),\n",
    "    train_feats_text_df.reset_index(drop=True)\n",
    "],axis=1)\n",
    "train_feats.drop('cleaned_text',axis=1,inplace=True)\n",
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 9143)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats_text_df = pd.DataFrame(test_feats_text.toarray(), columns=vectorizer.get_feature_names())\n",
    "test_feats_text_df = test_feats_text_df.add_prefix('LSA_')\n",
    "test_feats = pd.concat([\n",
    "    test_feats.reset_index(drop=True),\n",
    "    test_feats_text_df.reset_index(drop=True)\n",
    "],axis=1)\n",
    "test_feats.drop('cleaned_text',axis=1,inplace=True)\n",
    "test_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to keep track of scoring and for printing results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "def scoring(score_dict,train_feats,train_labels,test_feats,test_labels,clf):\n",
    "    if 'accuracy' not in score_dict:\n",
    "        score_dict['accuracy'] = []\n",
    "    if 'f1' not in score_dict:\n",
    "        score_dict['f1'] = []\n",
    "    if 'recall' not in score_dict:\n",
    "        score_dict['recall'] = []    \n",
    "    if 'FN' not in score_dict:\n",
    "        score_dict['FN'] = []\n",
    "    clf_name = re.findall('(^[^\\(]+)\\(',str(clf))[0]\n",
    "    already_seen = clf.score(train_feats, train_labels)\n",
    "    accuracy = clf.score(test_feats, test_labels)\n",
    "    pred = clf.predict(test_feats)\n",
    "    f1 = f1_score(test_labels, pred, pos_label='investigate')\n",
    "    recall = recall_score(test_labels, pred, pos_label='investigate')\n",
    "    cnf_matrix = confusion_matrix(test_labels, pred)\n",
    "    FN = cnf_matrix[1][0]\n",
    "    false_negative = cnf_matrix[1][0]\n",
    "    score_dict['accuracy'].append((clf_name,accuracy))\n",
    "    score_dict['f1'].append((clf_name,f1))\n",
    "    score_dict['recall'].append((clf_name,recall))\n",
    "    score_dict['FN'].append((clf_name,false_negative))\n",
    "    print(clf_name + ' Scores:\\n')\n",
    "    print('Accuracy of data already seen: %0.4f' % already_seen)\n",
    "    print('Accuracy of data not seen: %0.4f' % accuracy)\n",
    "    print('F1 score: %0.4f' % f1)\n",
    "    print('Recall score: %0f' % recall)\n",
    "    print('False Negatives: %0d' % FN)\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed:  8.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 495.284s\n",
      "0.9250398724082934\n",
      "max_depth: None\n",
      "max_leaf_nodes: 200\n",
      "min_impurity_decrease: 1e-05\n",
      "n_estimators: 50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "t0 = time()\n",
    "rf_clf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': (50, 100, 200),\n",
    "    'min_impurity_decrease': np.append(np.logspace(-6, -1, 6),[0,1,5,10]),\n",
    "    'max_leaf_nodes': (None, 100, 200, 400),\n",
    "    'max_depth': (None, 50, 100, 200),\n",
    "}\n",
    "gs_rf_clf = GridSearchCV(estimator=rf_clf, param_grid=parameters, n_jobs=-1,cv=5,verbose=True)\n",
    "gs_rf_clf.fit(train_feats, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(gs_rf_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_rf_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try with scaling the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://stackoverflow.com/a/37199623\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "train_feats = min_max_scaler.fit_transform(train_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 480 candidates, totalling 2400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   55.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2400 out of 2400 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 662.764s\n",
      "0.9282296650717703\n",
      "max_depth: 50\n",
      "max_leaf_nodes: 100\n",
      "min_impurity_decrease: 0.0\n",
      "n_estimators: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "t0 = time()\n",
    "rf_clf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': (50, 100, 200),\n",
    "    'min_impurity_decrease': np.append(np.logspace(-6, -1, 6),[0,1,5,10]),\n",
    "    'max_leaf_nodes': (None, 100, 200, 400),\n",
    "    'max_depth': (None, 50, 100, 200),\n",
    "}\n",
    "gs_rf_clf = GridSearchCV(estimator=rf_clf, param_grid=parameters, n_jobs=-1,cv=5,verbose=True)\n",
    "gs_rf_clf.fit(train_feats, train_labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(gs_rf_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_rf_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
