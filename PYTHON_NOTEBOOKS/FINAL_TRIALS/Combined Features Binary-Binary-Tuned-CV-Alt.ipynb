{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import email\n",
    "import re\n",
    "import mailparser\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from bs4 import BeautifulSoup\n",
    "from tld import get_tld\n",
    "from time import strftime, strptime, time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, recall_score\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set constants for directories, take a random sampling of the \"ignore\" email since there are so many (under sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_DIR = '/Users/worshamn/Documents/emailProject/IgnoreFix'\n",
    "INVESTIGATE_DIR = '/Users/worshamn/Documents/emailProject/InvestigateFix'\n",
    "#https://stackoverflow.com/a/6482922\n",
    "random.seed(2842)\n",
    "ignore_sample_index = [ i for i in sorted(random.sample(range(len(os.listdir(IGNORE_DIR))), 400)) ]\n",
    "ignore_sample = []\n",
    "for i in ignore_sample_index:\n",
    "    ignore_sample.append(os.listdir(IGNORE_DIR)[i])\n",
    "input_dirs = {}\n",
    "input_dirs[INVESTIGATE_DIR] = os.listdir(INVESTIGATE_DIR) \n",
    "input_dirs[IGNORE_DIR] = ignore_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Phish Alert] FW- Barbara.jones shrink your belly, 1lb day.eml',\n",
       "...\n",
       " '[Phish Alert] FW- How 4,500 Worksites Are Getting Healthier Right Now.eml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignore_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(input_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_dirs[INVESTIGATE_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_dirs[IGNORE_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to extract text and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_text(file):\n",
    "    d = {}\n",
    "    raw_message = email.message_from_file(file)\n",
    "    mail = mailparser.parse_from_string(raw_message.as_string())\n",
    "    d['subject'] = mail.subject\n",
    "    d['subject_len'] = len(d['subject'])\n",
    "    if raw_message.is_multipart():\n",
    "        d['is_mulitpart'] = 1\n",
    "    else:\n",
    "        d['is_multipart'] = 0\n",
    "    d['body'] = mail.text_plain\n",
    "    if len(d['body']) > 0:\n",
    "        d['mail_text'] = d['subject'] + ' ' + d['body'][0]\n",
    "        d['body_len'] = len(d['body'][0])\n",
    "        if len(d['body']) > 1:\n",
    "            soup_html = BeautifulSoup(d['body'][1],'lxml')\n",
    "            d['links'] = soup_html.find_all('a')\n",
    "            d['num_link'] = len(d['links'])\n",
    "            links = []\n",
    "            d['masq_link'] = []\n",
    "            d['masq_link_tld'] = []\n",
    "            d['num_email_link'] = 0\n",
    "            for link in d['links']:\n",
    "                link_text = link.get_text().rstrip('\\n')\n",
    "                a_link = link.get('href')\n",
    "                links.append(a_link)\n",
    "                if 'unsubscribe' in link_text.lower():\n",
    "                    d['has_unsubscribe_link'] = 1\n",
    "                if a_link:    \n",
    "                    if re.search('mailto:',a_link):\n",
    "                        d['num_email_link'] += 1\n",
    "                if a_link != link_text and \\\n",
    "                    'http' in link_text.lower() and \\\n",
    "                    not 'alt=\"http' in link_text.lower():\n",
    "                        d['masq_link'].append(link)\n",
    "                        d['masq_link_tld'].append(\n",
    "                            get_tld(\n",
    "                                a_link,\n",
    "                                fix_protocol=True, \n",
    "                                fail_silently=True\n",
    "                            )\n",
    "                        )\n",
    "            d['num_uniq_link'] = len(set(links))\n",
    "            if d['num_link'] > d['num_uniq_link']:\n",
    "                d['has_repeatlink'] = 1\n",
    "            else:\n",
    "                d['has_repeatlink'] = 0\n",
    "            if len(d['masq_link']) == 0:\n",
    "                d['masq_link'] = ''\n",
    "                d['masq_link_tld'] = ''\n",
    "                d['has_masq_link'] = 0\n",
    "            else:    \n",
    "                d['has_masq_link'] = 1\n",
    "                d['num_masq_link'] = len(d['masq_link'])\n",
    "    else:\n",
    "        d['mail_text'] = d['subject']\n",
    "        d['body_len'] = len(d['body'])\n",
    "    url_query = '((?:https?|ftp)://[^\\s/$.?#]+\\.[^\\s>]+)'\n",
    "    d['url'] = re.findall(url_query,d['mail_text'])\n",
    "    email_query = '([\\w.]+@[\\w.]+\\.[\\w.]{2,5})'\n",
    "    d['email'] = re.findall(email_query,d['mail_text'])\n",
    "    if d['url']:\n",
    "        d['has_url'] = 1\n",
    "        d['num_url'] = len(d['url'])\n",
    "        d['num_uniq_url'] = len(set(d['url']))\n",
    "        d['num_url_repeats'] = d['num_url'] - d['num_uniq_url']\n",
    "        d['url_len'] = []\n",
    "        d['url_tld'] = []\n",
    "        for i in d['url']:\n",
    "            d['url_len'].append(len(i))\n",
    "            d['url_tld'].append(\n",
    "                get_tld(i, fix_protocol=True, fail_silently=True)\n",
    "            )\n",
    "            d['uniq_url_tld'] = set(d['url_tld'])\n",
    "    else:\n",
    "        d['url'] = ''\n",
    "        d['has_url'] = 0\n",
    "        d['num_url'] = 0\n",
    "        d['num_uniq_url'] = 0\n",
    "        d['url_len'] = 0\n",
    "        d['url_tld'] = 0\n",
    "        d['uniq_url_tld'] = 0\n",
    "        d['num_url_repeats'] = 0\n",
    "    if d['email']:\n",
    "        d['has_email'] = 1\n",
    "        d['num_email_addr'] = len(d['email'])\n",
    "        d['num_uniq_email'] = len(set(d['email']))\n",
    "    else:\n",
    "        d['email'] = ''\n",
    "        d['has_email'] = 0\n",
    "        d['num_email_addr'] = 0\n",
    "        d['num_uniq_email'] = 0\n",
    "    soup = BeautifulSoup(d['mail_text'],'lxml')\n",
    "    try:\n",
    "        d['mail_text'] = soup.get_text().encode(\n",
    "            'utf-8',\n",
    "            'ignore'\n",
    "        ).decode('unicode_escape').replace('\\n',' ').replace('\\t',' ')\n",
    "    except:\n",
    "        d['mail_text'] = soup.get_text().replace('\\n',' ').replace('\\t',' ')\n",
    "    d['reply_to'] = mail.reply_to\n",
    "    for k,v in mail.headers.items():\n",
    "        d[k] = v\n",
    "    d['body'] = mail.body\n",
    "    d['text_plain'] = mail.text_plain\n",
    "    if mail.attachments:\n",
    "        d['has_attachments'] = 1\n",
    "        d['num_attachments'] = len(mail.attachments)\n",
    "        for i in mail.attachments:\n",
    "            try:\n",
    "                d['attachment_filename'].append(i['filename'])\n",
    "            except:\n",
    "                d['attachment_filename'] = []\n",
    "                d['attachment_filename'].append(i['filename'])\n",
    "            try:\n",
    "                d['attachment_content_type'].append(i['mail_content_type'])\n",
    "            except:\n",
    "                d['attachment_content_type'] = []\n",
    "                d['attachment_content_type'].append(i['mail_content_type'])\n",
    "    else:\n",
    "        d['has_attachments'] = 0\n",
    "    if re.search(tryGetKeyValue(d, 'Return-Path'),tryGetKeyValue(d, 'From')):\n",
    "        d['return_path_match_from'] = 1\n",
    "    else:\n",
    "        d['return_path_match_from'] = 0\n",
    "    tld_match = re.match(\n",
    "            '[^@]+@([^>]+)>',\n",
    "            tryGetKeyValue(d, 'From')\n",
    "        )\n",
    "    if tld_match:\n",
    "        d['from_tld'] = get_tld(\n",
    "            tld_match.group(1),\n",
    "            fix_protocol=True, \n",
    "            fail_silently=True\n",
    "        )\n",
    "    if 'content=\"text/html' in d['body'].lower():\n",
    "        d['has_html_content'] = 1\n",
    "    else:\n",
    "        d['has_html_content'] = 0\n",
    "    if 'script type=\"text/javascript' in d['body'].lower():\n",
    "        d['has_javascript'] = 1\n",
    "    else:\n",
    "        d['has_javascript'] = 0\n",
    "    if 'img src=\"cid:' in d['body'].lower():\n",
    "        d['has_inline_img'] = 1\n",
    "    else:\n",
    "        d['has_inline_img'] = 0\n",
    "    if 'Content-type' in d:\n",
    "        d['Content-type'] = re.match('([^;]+);',d['Content-type']).group(1)\n",
    "    else:\n",
    "        d['Content-type'] = None\n",
    "    if 'Date' in d:\n",
    "        #d['DOTW'] = strftime('%a',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "        d['DOTW'] = strftime('%w',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "        d['HOTD'] = strftime('%H',strptime(d['Date'],'%a, %d %b %Y %H:%M:%S %z'))\n",
    "    if mail.has_defects:\n",
    "        d['has_defects'] = 1\n",
    "    else:\n",
    "        d['has_defects'] = 0\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to set the target value based on the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(d):\n",
    "    if d == IGNORE_DIR:\n",
    "        return 'ignore'\n",
    "    elif d == INVESTIGATE_DIR:\n",
    "        return 'investigate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build function to extract value only if the key exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryGetKeyValue(d, key, return_value=''):\n",
    "  \"\"\"Attempts to return value of key from dictionary\n",
    "  \"\"\"\n",
    "  try:\n",
    "    return d[key]\n",
    "  except:\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe, iterate through directories and add email features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/worshamn/Documents/emailProject/InvestigateFix [Phish Alert] FW- Goodchem Document.eml\n",
       "...\n",
      "/Users/worshamn/Documents/emailProject/IgnoreFix [Phish Alert] FW- How 4,500 Worksites Are Getting Healthier Right Now.eml\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for d,v in input_dirs.items():\n",
    "    for f in input_dirs[d]:\n",
    "        print(d, f)\n",
    "        with open(os.path.join(d,f),'r',encoding='utf-8', errors='ignore') as raw_mail:\n",
    "            mail_dict = get_email_text(raw_mail)\n",
    "            df = df.append(\n",
    "                {\n",
    "                    #'filename': f,\n",
    "                    'text': mail_dict['mail_text'],\n",
    "                    'target': get_target(d),\n",
    "                    'subject_len': mail_dict['subject_len'],\n",
    "                    'body_len': mail_dict['body_len'],\n",
    "                    'has_attachments': tryGetKeyValue(mail_dict, 'has_attachments',0),\n",
    "                    'num_attachments': tryGetKeyValue(mail_dict, 'num_attachments',0),\n",
    "                    #'attachment_filename': tryGetKeyValue(mail_dict, 'attachment_filename'),\n",
    "                    #'attachment_content_type': tryGetKeyValue(mail_dict, 'attachment_content_type'),\n",
    "                    'DKIM': tryGetKeyValue(mail_dict, 'X-BAEAI-DKIM'),\n",
    "                    'DMARC': tryGetKeyValue(mail_dict, 'X-BAEAI-DMARC'),\n",
    "                    'SPF': tryGetKeyValue(mail_dict, 'X-BAEAI-SPF'),\n",
    "                    'return_path_match_from': mail_dict['return_path_match_from'],\n",
    "                    'from_tld': tryGetKeyValue(mail_dict, 'from_tld'),\n",
    "                    'Content-type': mail_dict['Content-type'],\n",
    "                    'DOTW': tryGetKeyValue(mail_dict, 'DOTW'),\n",
    "                    'HOTD': tryGetKeyValue(mail_dict, 'HOTD'),\n",
    "                    #'url': mail_dict['url'],\n",
    "                    'has_url': tryGetKeyValue(mail_dict, 'has_url',0),\n",
    "                    'num_url': tryGetKeyValue(mail_dict, 'num_url',0),\n",
    "                    'num_uniq_url': tryGetKeyValue(mail_dict, 'num_uniq_url',0),\n",
    "                    #'email': tryGetKeyValue(mail_dict, 'email'),\n",
    "                    'has_email': tryGetKeyValue(mail_dict, 'has_email',0),\n",
    "                    'num_email_addr': tryGetKeyValue(mail_dict, 'num_email_addr',0),\n",
    "                    'num_uniq_email': tryGetKeyValue(mail_dict, 'num_uniq_email',0),\n",
    "                    'num_url_repeats': tryGetKeyValue(mail_dict, 'num_url_repeats',0),\n",
    "                    #'url_len': mail_dict['url_len'],\n",
    "                    #'url_tld': mail_dict['url_tld'],\n",
    "                    #'uniq_url_tld': mail_dict['uniq_url_tld'],\n",
    "                    'has_html_content': tryGetKeyValue(mail_dict, 'has_html_content',0),\n",
    "                    'has_javascript': tryGetKeyValue(mail_dict, 'has_javascript',0),\n",
    "                    'has_inline_img': tryGetKeyValue(mail_dict, 'has_inline_img',0),\n",
    "                    'TAP-Score': tryGetKeyValue(mail_dict, 'X-USANET-TAP-Score',-1),\n",
    "                    #'links': tryGetKeyValue(mail_dict, 'links'),\n",
    "                    'num_link': tryGetKeyValue(mail_dict, 'num_link',0),\n",
    "                    'num_uniq_link': tryGetKeyValue(mail_dict, 'num_uniq_link',0),\n",
    "                    'has_repeat_link': tryGetKeyValue(mail_dict, 'has_repeat_link',0),\n",
    "                    #'masq_link': tryGetKeyValue(mail_dict, 'masq_link'),\n",
    "                    'has_masq_link': tryGetKeyValue(mail_dict, 'has_masq_link',0),\n",
    "                    'num_masq_link': tryGetKeyValue(mail_dict, 'num_masq_link',0),\n",
    "                    #'masq_link_tld': tryGetKeyValue(mail_dict, 'masq_link_tld'),\n",
    "                    'is_multipart': tryGetKeyValue(mail_dict, 'is_mulitpart', 0),\n",
    "                    'has_defects': mail_dict['has_defects'],\n",
    "                    'num_email_link': tryGetKeyValue(mail_dict, 'num_email_link',0),\n",
    "                    'has_unsubscribe_link': tryGetKeyValue(mail_dict, 'has_unsubscribe_link', 0),\n",
    "                }, \n",
    "                ignore_index=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 34 columns):\n",
      "Content-type              773 non-null object\n",
      "DKIM                      784 non-null object\n",
      "DMARC                     784 non-null object\n",
      "DOTW                      784 non-null object\n",
      "HOTD                      784 non-null object\n",
      "SPF                       784 non-null object\n",
      "TAP-Score                 784 non-null object\n",
      "body_len                  784 non-null float64\n",
      "from_tld                  784 non-null object\n",
      "has_attachments           784 non-null float64\n",
      "has_defects               784 non-null float64\n",
      "has_email                 784 non-null float64\n",
      "has_html_content          784 non-null float64\n",
      "has_inline_img            784 non-null float64\n",
      "has_javascript            784 non-null float64\n",
      "has_masq_link             784 non-null float64\n",
      "has_repeat_link           784 non-null float64\n",
      "has_unsubscribe_link      784 non-null float64\n",
      "has_url                   784 non-null float64\n",
      "is_multipart              784 non-null float64\n",
      "num_attachments           784 non-null float64\n",
      "num_email_addr            784 non-null float64\n",
      "num_email_link            784 non-null float64\n",
      "num_link                  784 non-null float64\n",
      "num_masq_link             784 non-null float64\n",
      "num_uniq_email            784 non-null float64\n",
      "num_uniq_link             784 non-null float64\n",
      "num_uniq_url              784 non-null float64\n",
      "num_url                   784 non-null float64\n",
      "num_url_repeats           784 non-null float64\n",
      "return_path_match_from    784 non-null float64\n",
      "subject_len               784 non-null float64\n",
      "target                    784 non-null object\n",
      "text                      784 non-null object\n",
      "dtypes: float64(24), object(10)\n",
      "memory usage: 208.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(\n",
    "    docs, \n",
    "    remove_urls=True,\n",
    "    remove_emails=True,\n",
    "    lemmatize=True,\n",
    "    remove_stopwords=True, \n",
    "    custom_stopwords=None,\n",
    "#     term_min_len=0,\n",
    "):\n",
    "    #only use parts of spaCy needed\n",
    "    nlp = spacy.load('en', disable=['parser','ner','textcat'])\n",
    "    #remove urls\n",
    "    if remove_urls:\n",
    "        print('remove URLS')\n",
    "        docs = [\n",
    "            re.sub('(?i)(?:www|https?)(?:://)?[^\\s]+','',text)\n",
    "            for text in docs\n",
    "        ]\n",
    "    #remove emails\n",
    "    if remove_emails:\n",
    "        print('remove email addresses')\n",
    "        docs = [\n",
    "            re.sub('(?i)[\\w.]+@[\\w.]+\\.[\\w.]{2,5}','',text)\n",
    "            for text in docs\n",
    "        ]\n",
    "    #remove punct and digits\n",
    "    print('removing punctuation and digits and change to lowercase')\n",
    "    table = str.maketrans({key: None for key in string.punctuation + string.digits})\n",
    "    clean_docs = [\n",
    "        str(d).lower().translate(table)\n",
    "        for d in docs\n",
    "    ]\n",
    "        \n",
    "    #tokenize in spacy\n",
    "    if lemmatize:\n",
    "        print('spacy tokenization')\n",
    "        nlp_docs = [nlp(d) for d in clean_docs]\n",
    "        #lemmatization, words like I get changed into -PRON- so leave them alone\n",
    "        if remove_stopwords:\n",
    "            print('lemmatization and remove stopwords')\n",
    "            if custom_stopwords:\n",
    "                custom_stopwords = set(custom_stopwords)\n",
    "            else:\n",
    "                custom_stopwords = []\n",
    "            lemmatized_docs = [\n",
    "                [\n",
    "                    w.lemma_ \n",
    "                    for w in d\n",
    "                    if (w.lemma_ != '-PRON-' and not w.is_stop and w.lemma_ not in custom_stopwords) \n",
    "                ]\n",
    "                for d in nlp_docs\n",
    "            ]\n",
    "        else:\n",
    "            print('lemmatization')\n",
    "            lemmatized_docs = [\n",
    "            [\n",
    "                w.lemma_\n",
    "                if w.lemma_ != '-PRON-'\n",
    "                else w.lower_\n",
    "                for w in d\n",
    "                #if (w.lemma_ != '-PRON-' and len(w.lemma_)>term_min_len)\n",
    "            ]\n",
    "            for d in nlp_docs\n",
    "        ]\n",
    "    if lemmatized_docs:\n",
    "        clean_docs = lemmatized_docs\n",
    "    \n",
    "    # join tokens back into doc\n",
    "    clean_docs = [\n",
    "        ' '.join(l) \n",
    "        for l in clean_docs\n",
    "    ]\n",
    "\n",
    "    return clean_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove URLS\n",
      "remove email addresses\n",
      "removing punctuation and digits and change to lowercase\n",
      "spacy tokenization\n",
      "lemmatization and remove stopwords\n",
      "done in 10.424s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "corpus = clean_text(\n",
    "    df['text'], \n",
    ")\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = pd.Series(corpus).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert cells that are not recognized as a number or just to get rid of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/28910914\n",
    "for col in [\n",
    "    'body_len',\n",
    "    'has_attachments',\n",
    "    'has_defects',\n",
    "    'has_email',\n",
    "    'has_html_content',\n",
    "    'has_inline_img',\n",
    "    'has_javascript',\n",
    "    'has_masq_link',\n",
    "    'has_repeat_link',\n",
    "    'has_unsubscribe_link',\n",
    "    'has_url',\n",
    "    'is_multipart',\n",
    "    'num_attachments',\n",
    "    'num_email_addr',\n",
    "    'num_email_link',\n",
    "    'num_link',\n",
    "    'num_masq_link',\n",
    "    'num_uniq_email',\n",
    "    'num_uniq_link',\n",
    "    'num_uniq_url',\n",
    "    'num_url',\n",
    "    'num_url_repeats',\n",
    "    'return_path_match_from',\n",
    "    'subject_len',\n",
    "    #'TAP-Score',\n",
    "]:\n",
    "    df[col] = df[col].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 784 entries, 0 to 783\n",
      "Data columns (total 35 columns):\n",
      "Content-type              773 non-null object\n",
      "DKIM                      784 non-null object\n",
      "DMARC                     784 non-null object\n",
      "DOTW                      784 non-null object\n",
      "HOTD                      784 non-null object\n",
      "SPF                       784 non-null object\n",
      "TAP-Score                 784 non-null object\n",
      "body_len                  784 non-null int64\n",
      "from_tld                  784 non-null object\n",
      "has_attachments           784 non-null int64\n",
      "has_defects               784 non-null int64\n",
      "has_email                 784 non-null int64\n",
      "has_html_content          784 non-null int64\n",
      "has_inline_img            784 non-null int64\n",
      "has_javascript            784 non-null int64\n",
      "has_masq_link             784 non-null int64\n",
      "has_repeat_link           784 non-null int64\n",
      "has_unsubscribe_link      784 non-null int64\n",
      "has_url                   784 non-null int64\n",
      "is_multipart              784 non-null int64\n",
      "num_attachments           784 non-null int64\n",
      "num_email_addr            784 non-null int64\n",
      "num_email_link            784 non-null int64\n",
      "num_link                  784 non-null int64\n",
      "num_masq_link             784 non-null int64\n",
      "num_uniq_email            784 non-null int64\n",
      "num_uniq_link             784 non-null int64\n",
      "num_uniq_url              784 non-null int64\n",
      "num_url                   784 non-null int64\n",
      "num_url_repeats           784 non-null int64\n",
      "return_path_match_from    784 non-null int64\n",
      "subject_len               784 non-null int64\n",
      "target                    784 non-null object\n",
      "text                      784 non-null object\n",
      "cleaned_text              784 non-null object\n",
      "dtypes: int64(24), object(11)\n",
      "memory usage: 214.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break the continous data into ranges for binary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/40548606\n",
    "df['body_len'] = pd.qcut(df['body_len'],20,duplicates='drop')\n",
    "df['subject_len'] = pd.qcut(df['subject_len'],10,duplicates='drop')\n",
    "df['num_attachments'] = pd.cut(df['num_attachments'],5)\n",
    "df['num_email_addr'] = pd.cut(df['num_email_addr'],5)\n",
    "df['num_email_link'] = pd.cut(df['num_email_link'],5)\n",
    "df['num_link'] = pd.qcut(df['num_link'],10,duplicates='drop')\n",
    "df['num_masq_link'] = pd.cut(df['num_masq_link'],3)\n",
    "df['num_uniq_email'] = pd.qcut(df['num_uniq_email'],10,duplicates='drop')\n",
    "df['num_uniq_url'] = pd.qcut(df['num_uniq_url'],10,duplicates='drop')\n",
    "df['num_url'] = pd.qcut(df['num_url'],10,duplicates='drop')\n",
    "df['num_url_repeats'] = pd.qcut(df['num_url_repeats'],10,duplicates='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the continuous and categorical data into one-hot encoding (binary data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    'DKIM',\n",
    "    'DMARC',\n",
    "    'SPF',\n",
    "    'from_tld',\n",
    "    'Content-type',\n",
    "    'DOTW',\n",
    "    'HOTD',\n",
    "    'TAP-Score',\n",
    "    'body_len',\n",
    "    'subject_len',\n",
    "    'num_attachments',\n",
    "    'num_email_addr',\n",
    "    'num_email_link',\n",
    "    'num_link',\n",
    "    'num_masq_link',\n",
    "    'num_uniq_email',\n",
    "    'num_uniq_url',\n",
    "    'num_url',\n",
    "    'num_url_repeats',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = pd.get_dummies(df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(categorical_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,df_categorical], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_attachments</th>\n",
       "      <th>has_defects</th>\n",
       "      <th>has_email</th>\n",
       "      <th>has_html_content</th>\n",
       "      <th>has_inline_img</th>\n",
       "      <th>has_javascript</th>\n",
       "      <th>has_masq_link</th>\n",
       "      <th>has_repeat_link</th>\n",
       "      <th>has_unsubscribe_link</th>\n",
       "      <th>has_url</th>\n",
       "      <th>...</th>\n",
       "      <th>num_url_(-0.001, 1.0]</th>\n",
       "      <th>num_url_(1.0, 2.0]</th>\n",
       "      <th>num_url_(2.0, 4.0]</th>\n",
       "      <th>num_url_(4.0, 6.0]</th>\n",
       "      <th>num_url_(6.0, 11.0]</th>\n",
       "      <th>num_url_(11.0, 17.0]</th>\n",
       "      <th>num_url_(17.0, 341.0]</th>\n",
       "      <th>num_url_repeats_(-0.001, 1.0]</th>\n",
       "      <th>num_url_repeats_(1.0, 3.0]</th>\n",
       "      <th>num_url_repeats_(3.0, 95.0]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   has_attachments  has_defects  has_email  has_html_content  has_inline_img  \\\n",
       "0                1            0          1                 0               0   \n",
       "1                0            0          0                 1               0   \n",
       "2                0            0          0                 1               0   \n",
       "3                0            0          0                 0               0   \n",
       "4                0            0          0                 0               0   \n",
       "\n",
       "   has_javascript  has_masq_link  has_repeat_link  has_unsubscribe_link  \\\n",
       "0               0              0                0                     0   \n",
       "1               0              0                0                     0   \n",
       "2               0              1                0                     0   \n",
       "3               0              0                0                     0   \n",
       "4               0              0                0                     0   \n",
       "\n",
       "   has_url             ...               num_url_(-0.001, 1.0]  \\\n",
       "0        1             ...                                   0   \n",
       "1        1             ...                                   0   \n",
       "2        1             ...                                   0   \n",
       "3        1             ...                                   1   \n",
       "4        1             ...                                   1   \n",
       "\n",
       "   num_url_(1.0, 2.0]  num_url_(2.0, 4.0] num_url_(4.0, 6.0]  \\\n",
       "0                   0                   0                  0   \n",
       "1                   1                   0                  0   \n",
       "2                   1                   0                  0   \n",
       "3                   0                   0                  0   \n",
       "4                   0                   0                  0   \n",
       "\n",
       "  num_url_(6.0, 11.0] num_url_(11.0, 17.0]  num_url_(17.0, 341.0]  \\\n",
       "0                   1                    0                      0   \n",
       "1                   0                    0                      0   \n",
       "2                   0                    0                      0   \n",
       "3                   0                    0                      0   \n",
       "4                   0                    0                      0   \n",
       "\n",
       "   num_url_repeats_(-0.001, 1.0]  num_url_repeats_(1.0, 3.0]  \\\n",
       "0                              1                           0   \n",
       "1                              1                           0   \n",
       "2                              1                           0   \n",
       "3                              1                           0   \n",
       "4                              1                           0   \n",
       "\n",
       "   num_url_repeats_(3.0, 95.0]  \n",
       "0                            0  \n",
       "1                            0  \n",
       "2                            0  \n",
       "3                            0  \n",
       "4                            0  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_attachments\n",
      "has_defects\n",
      "has_email\n",
      "has_html_content\n",
      "has_inline_img\n",
      "has_javascript\n",
      "has_masq_link\n",
      "has_repeat_link\n",
      "has_unsubscribe_link\n",
      "has_url\n",
      "is_multipart\n",
      "num_uniq_link\n",
      "return_path_match_from\n",
      "target\n",
      "text\n",
      "cleaned_text\n",
      "DKIM_\n",
      "DKIM_FAIL\n",
      "DKIM_NONE\n",
      "DKIM_PASS\n",
      "DMARC_\n",
      "DMARC_absent\n",
      "DMARC_none\n",
      "DMARC_pass\n",
      "DMARC_quarantine\n",
      "DMARC_reject\n",
      "SPF_\n",
      "SPF_ERROR\n",
      "SPF_NEUTRAL\n",
      "SPF_NONE\n",
      "SPF_PASS\n",
      "SPF_SOFTFAIL\n",
      "from_tld_\n",
      "from_tld_ac.uk\n",
      "from_tld_bg\n",
      "from_tld_bid\n",
      "from_tld_biz\n",
      "from_tld_ca\n",
      "from_tld_ch\n",
      "from_tld_co\n",
      "from_tld_co.jp\n",
      "from_tld_co.uk\n",
      "from_tld_co.za\n",
      "from_tld_com\n",
      "from_tld_com.ar\n",
      "from_tld_com.br\n",
      "from_tld_com.do\n",
      "from_tld_com.ec\n",
      "from_tld_com.mx\n",
      "from_tld_com.my\n",
      "from_tld_com.pk\n",
      "from_tld_de\n",
      "from_tld_edu\n",
      "from_tld_email\n",
      "from_tld_es\n",
      "from_tld_eu\n",
      "from_tld_fi\n",
      "from_tld_fr\n",
      "from_tld_gob.cl\n",
      "from_tld_gob.ec\n",
      "from_tld_gov\n",
      "from_tld_gr\n",
      "from_tld_guru\n",
      "from_tld_hu\n",
      "from_tld_ie\n",
      "from_tld_info\n",
      "from_tld_io\n",
      "from_tld_it\n",
      "from_tld_k12.in.us\n",
      "from_tld_k12.wi.us\n",
      "from_tld_mx\n",
      "from_tld_ne.jp\n",
      "from_tld_net\n",
      "from_tld_net.pl\n",
      "from_tld_net.tw\n",
      "from_tld_org\n",
      "from_tld_org.br\n",
      "from_tld_ph\n",
      "from_tld_pl\n",
      "from_tld_ro\n",
      "from_tld_ru\n",
      "from_tld_se\n",
      "from_tld_sk\n",
      "from_tld_travel\n",
      "from_tld_tv\n",
      "from_tld_uk\n",
      "from_tld_us\n",
      "Content-type_Multipart/alternative\n",
      "Content-type_Multipart/mixed\n",
      "Content-type_Multipart/related\n",
      "Content-type_text/plain\n",
      "DOTW_\n",
      "DOTW_0\n",
      "DOTW_1\n",
      "DOTW_2\n",
      "DOTW_3\n",
      "DOTW_4\n",
      "DOTW_5\n",
      "DOTW_6\n",
      "HOTD_\n",
      "HOTD_01\n",
      "HOTD_02\n",
      "HOTD_03\n",
      "HOTD_04\n",
      "HOTD_05\n",
      "HOTD_06\n",
      "HOTD_07\n",
      "HOTD_08\n",
      "HOTD_09\n",
      "HOTD_10\n",
      "HOTD_11\n",
      "HOTD_12\n",
      "HOTD_13\n",
      "HOTD_14\n",
      "HOTD_15\n",
      "HOTD_16\n",
      "HOTD_17\n",
      "HOTD_18\n",
      "HOTD_19\n",
      "HOTD_20\n",
      "HOTD_21\n",
      "HOTD_22\n",
      "HOTD_23\n",
      "TAP-Score_-1\n",
      "TAP-Score_0\n",
      "TAP-Score_1\n",
      "TAP-Score_2\n",
      "TAP-Score_3\n",
      "body_len_(8.999, 111.15]\n",
      "body_len_(111.15, 166.3]\n",
      "body_len_(166.3, 207.45]\n",
      "body_len_(207.45, 237.6]\n",
      "body_len_(237.6, 273.75]\n",
      "body_len_(273.75, 335.9]\n",
      "body_len_(335.9, 422.1]\n",
      "body_len_(422.1, 513.2]\n",
      "body_len_(513.2, 679.15]\n",
      "body_len_(679.15, 949.5]\n",
      "body_len_(949.5, 1179.0]\n",
      "body_len_(1179.0, 1386.6]\n",
      "body_len_(1386.6, 1640.7]\n",
      "body_len_(1640.7, 2000.2]\n",
      "body_len_(2000.2, 2354.5]\n",
      "body_len_(2354.5, 2861.6]\n",
      "body_len_(2861.6, 3923.2]\n",
      "body_len_(3923.2, 5470.1]\n",
      "body_len_(5470.1, 8140.15]\n",
      "body_len_(8140.15, 276506.0]\n",
      "subject_len_(-0.001, 13.0]\n",
      "subject_len_(13.0, 19.0]\n",
      "subject_len_(19.0, 24.0]\n",
      "subject_len_(24.0, 30.0]\n",
      "subject_len_(30.0, 35.0]\n",
      "subject_len_(35.0, 40.0]\n",
      "subject_len_(40.0, 46.0]\n",
      "subject_len_(46.0, 53.0]\n",
      "subject_len_(53.0, 62.0]\n",
      "subject_len_(62.0, 246.0]\n",
      "num_attachments_(-0.015, 3.0]\n",
      "num_attachments_(3.0, 6.0]\n",
      "num_attachments_(6.0, 9.0]\n",
      "num_attachments_(9.0, 12.0]\n",
      "num_attachments_(12.0, 15.0]\n",
      "num_email_addr_(-0.193, 38.6]\n",
      "num_email_addr_(38.6, 77.2]\n",
      "num_email_addr_(77.2, 115.8]\n",
      "num_email_addr_(115.8, 154.4]\n",
      "num_email_addr_(154.4, 193.0]\n",
      "num_email_link_(-0.103, 20.6]\n",
      "num_email_link_(20.6, 41.2]\n",
      "num_email_link_(41.2, 61.8]\n",
      "num_email_link_(61.8, 82.4]\n",
      "num_email_link_(82.4, 103.0]\n",
      "num_link_(-0.001, 1.0]\n",
      "num_link_(1.0, 2.0]\n",
      "num_link_(2.0, 3.0]\n",
      "num_link_(3.0, 6.0]\n",
      "num_link_(6.0, 9.0]\n",
      "num_link_(9.0, 14.0]\n",
      "num_link_(14.0, 339.0]\n",
      "num_masq_link_(-0.002, 0.667]\n",
      "num_masq_link_(0.667, 1.333]\n",
      "num_masq_link_(1.333, 2.0]\n",
      "num_uniq_email_(-0.001, 1.0]\n",
      "num_uniq_email_(1.0, 2.0]\n",
      "num_uniq_email_(2.0, 94.0]\n",
      "num_uniq_url_(-0.001, 1.0]\n",
      "num_uniq_url_(1.0, 2.0]\n",
      "num_uniq_url_(2.0, 3.0]\n",
      "num_uniq_url_(3.0, 6.0]\n",
      "num_uniq_url_(6.0, 9.0]\n",
      "num_uniq_url_(9.0, 15.0]\n",
      "num_uniq_url_(15.0, 341.0]\n",
      "num_url_(-0.001, 1.0]\n",
      "num_url_(1.0, 2.0]\n",
      "num_url_(2.0, 4.0]\n",
      "num_url_(4.0, 6.0]\n",
      "num_url_(6.0, 11.0]\n",
      "num_url_(11.0, 17.0]\n",
      "num_url_(17.0, 341.0]\n",
      "num_url_repeats_(-0.001, 1.0]\n",
      "num_url_repeats_(1.0, 3.0]\n",
      "num_url_repeats_(3.0, 95.0]\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sample set into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df['target']\n",
    "df_feats = df.drop(['target','text'],axis=1)\n",
    "# train_feats, test_feats, train_labels, test_labels = train_test_split(\n",
    "#     df_feats, \n",
    "#     df_target, \n",
    "#     test_size=0.20, \n",
    "#     random_state=7350\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 201)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ignore         400\n",
       "investigate    384\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.564s\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1,3),\n",
    "    #max_df=0.9, \n",
    "    min_df=2,\n",
    "    use_idf=False,\n",
    "    binary=True,\n",
    ")\n",
    "df_feats_text = vectorizer.fit_transform(df_feats['cleaned_text'].values.tolist())\n",
    "# test_feats_text = vectorizer.transform(test_feats['cleaned_text'].values.tolist()) \n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 11898)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feats_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feats_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change TFIDF back to dataframe so it can be concatinated with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/50624143\n",
    "df_feats_text_df = pd.DataFrame(df_feats_text.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abbuchung</th>\n",
       "      <th>abend</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able access</th>\n",
       "      <th>able hold</th>\n",
       "      <th>able hold phone</th>\n",
       "      <th>able manufacture</th>\n",
       "      <th>...</th>\n",
       "      <th>zailab</th>\n",
       "      <th>zailab platinum</th>\n",
       "      <th>zailab platinum sponsor</th>\n",
       "      <th>zailab register</th>\n",
       "      <th>zip</th>\n",
       "      <th>zip code</th>\n",
       "      <th>zipongo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom discount</th>\n",
       "      <th>zoom discount base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 11898 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa   ab  abbuchung  abend  ability  able  able access  able hold  \\\n",
       "0  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "1  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "2  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "3  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "4  0.0  0.0        0.0    0.0      0.0   0.0          0.0        0.0   \n",
       "\n",
       "   able hold phone  able manufacture         ...          zailab  \\\n",
       "0              0.0               0.0         ...             0.0   \n",
       "1              0.0               0.0         ...             0.0   \n",
       "2              0.0               0.0         ...             0.0   \n",
       "3              0.0               0.0         ...             0.0   \n",
       "4              0.0               0.0         ...             0.0   \n",
       "\n",
       "   zailab platinum  zailab platinum sponsor  zailab register  zip  zip code  \\\n",
       "0              0.0                      0.0              0.0  0.0       0.0   \n",
       "1              0.0                      0.0              0.0  0.0       0.0   \n",
       "2              0.0                      0.0              0.0  0.0       0.0   \n",
       "3              0.0                      0.0              0.0  0.0       0.0   \n",
       "4              0.0                      0.0              0.0  0.0       0.0   \n",
       "\n",
       "   zipongo  zoom  zoom discount  zoom discount base  \n",
       "0      0.0   0.0            0.0                 0.0  \n",
       "1      0.0   0.0            0.0                 0.0  \n",
       "2      0.0   0.0            0.0                 0.0  \n",
       "3      0.0   0.0            0.0                 0.0  \n",
       "4      0.0   0.0            0.0                 0.0  \n",
       "\n",
       "[5 rows x 11898 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feats_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 12098)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feats = pd.concat([\n",
    "    df_feats.reset_index(drop=True),\n",
    "    df_feats_text_df.reset_index(drop=True)\n",
    "],axis=1)\n",
    "df_feats.drop('cleaned_text',axis=1,inplace=True)\n",
    "df_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_feats_text_df = pd.DataFrame(test_feats_text.toarray(), columns=vectorizer.get_feature_names())\n",
    "# test_feats_text_df = test_feats_text_df.add_prefix('LSA_')\n",
    "# test_feats = pd.concat([\n",
    "#     test_feats.reset_index(drop=True),\n",
    "#     test_feats_text_df.reset_index(drop=True)\n",
    "# ],axis=1)\n",
    "# test_feats.drop('cleaned_text',axis=1,inplace=True)\n",
    "# test_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to keep track of scoring and for printing results out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_dict = {}\n",
    "# def scoring(score_dict,df_feats,df_labels,test_feats,test_labels,clf):\n",
    "#     if 'accuracy' not in score_dict:\n",
    "#         score_dict['accuracy'] = []\n",
    "#     if 'f1' not in score_dict:\n",
    "#         score_dict['f1'] = []\n",
    "#     if 'recall' not in score_dict:\n",
    "#         score_dict['recall'] = []    \n",
    "#     if 'FN' not in score_dict:\n",
    "#         score_dict['FN'] = []\n",
    "#     clf_name = re.findall('(^[^\\(]+)\\(',str(clf))[0]\n",
    "#     already_seen = clf.score(df_feats, df_labels)\n",
    "#     accuracy = clf.score(test_feats, test_labels)\n",
    "#     pred = clf.predict(test_feats)\n",
    "#     f1 = f1_score(test_labels, pred, pos_label='investigate')\n",
    "#     recall = recall_score(test_labels, pred, pos_label='investigate')\n",
    "#     cnf_matrix = confusion_matrix(test_labels, pred)\n",
    "#     FN = cnf_matrix[1][0]\n",
    "#     false_negative = cnf_matrix[1][0]\n",
    "#     score_dict['accuracy'].append((clf_name,accuracy))\n",
    "#     score_dict['f1'].append((clf_name,f1))\n",
    "#     score_dict['recall'].append((clf_name,recall))\n",
    "#     score_dict['FN'].append((clf_name,false_negative))\n",
    "#     print(clf_name + ' Scores:\\n')\n",
    "#     print('Accuracy of data already seen: %0.4f' % already_seen)\n",
    "#     print('Accuracy of data not seen: %0.4f' % accuracy)\n",
    "#     print('F1 score: %0.4f' % f1)\n",
    "#     print('Recall score: %0f' % recall)\n",
    "#     print('False Negatives: %0d' % FN)\n",
    "#     return score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_feats_scaled = min_max_scaler.fit_transform(df_feats)\n",
    "# test_feats_scaled = min_max_scaler.transform(test_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.000s\n",
      "[0.96202532 0.94936709 0.97468354 0.93670886 0.96153846 0.93589744\n",
      " 0.87179487 0.96153846 0.94871795 0.8974359 ]\n",
      "Accuracy: 0.9400 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "svm_clf = LinearSVC(random_state=7350,C=0.1,loss='hinge',tol=1e-10)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "svm_scores = cross_val_score(svm_clf, df_feats_scaled, df_target, cv=10)\n",
    "print(svm_scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (svm_scores.mean(), svm_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.000s\n",
      "[0.96202532 0.91139241 0.92405063 0.89873418 0.96153846 0.92307692\n",
      " 0.91025641 0.94871795 0.93589744 0.87179487]\n",
      "Accuracy: 0.9247 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# rf_clf = RandomForestClassifier(n_jobs=-1,n_estimators=100,random_state=7350,max_leaf_nodes=400)\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100,\n",
    "    random_state=7350,\n",
    "    max_leaf_nodes=100,\n",
    "    max_depth=50,\n",
    "    min_impurity_decrease=0,\n",
    ")\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "# score_dict = scoring(score_dict,df_feats_scaled,df_target,test_feats_scaled,test_target,rf_clf)\n",
    "et_scores = cross_val_score(rf_clf, df_feats_scaled, df_target, cv=10)\n",
    "print(et_scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (et_scores.mean(), et_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "# #https://stackoverflow.com/a/42429989\n",
    "# #https://stackoverflow.com/a/25219535\n",
    "# importances = rf_clf.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in rf_clf.estimators_],\n",
    "#              axis=0)\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "# features = train_feats.columns\n",
    "# print(\"Feature ranking:\")\n",
    "# for f in range(25):\n",
    "#     print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# top_indices = indices[:25]\n",
    "# top_features = [features[i] for i in indices[:25]]\n",
    "# plt.figure(figsize=(18, 16))\n",
    "# plt.title(\"Feature importances\")\n",
    "# plt.bar(range(25), importances[top_indices],color=\"orange\", yerr=std[top_indices], align=\"center\",)\n",
    "# plt.xticks(range(25), top_features, rotation=75)\n",
    "# plt.xlim([-1, 25])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.000s\n",
      "[0.96202532 0.94936709 0.97468354 0.92405063 0.97435897 0.94871795\n",
      " 0.8974359  0.96153846 0.94871795 0.91025641]\n",
      "Accuracy: 0.9451 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# nn_clf = MLPClassifier(max_iter=800,hidden_layer_sizes=(50, 100, 50),learning_rate='adaptive')\n",
    "nn_clf = MLPClassifier(max_iter=1200,hidden_layer_sizes=(50, 100, 50),learning_rate='adaptive')\n",
    "# nn_clf.fit(df_feats_scaled, df_target)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "# score_dict = scoring(score_dict,df_feats_scaled,df_target,test_feats_scaled,test_target,nn_clf)\n",
    "nn_scores = cross_val_score(nn_clf, df_feats_scaled, df_target, cv=10)\n",
    "print(nn_scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (nn_scores.mean(), nn_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.000s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96202532 0.94936709 0.96202532 0.93670886 0.96153846 0.93589744\n",
      " 0.88461538 0.94871795 0.94871795 0.8974359 ]\n",
      "Accuracy: 0.9387 (+/- 0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "vt_clf = VotingClassifier(estimators=[\n",
    "    ('et',rf_clf),\n",
    "    ('svm',svm_clf),\n",
    "    ('nn',nn_clf),\n",
    "],voting='hard')\n",
    "# vt_clf.fit(df_feats_scaled, df_target)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "vt_scores = cross_val_score(vt_clf, df_feats_scaled, df_target, cv=10)\n",
    "print(vt_scores)\n",
    "print(\"Accuracy: %0.4f (+/- %0.2f)\" % (vt_scores.mean(), vt_scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
